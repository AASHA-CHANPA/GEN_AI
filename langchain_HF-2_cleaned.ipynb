{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-huggingface\n",
    "!pip install huggingface_hub\n",
    "!pip install transformers\n",
    "!pip install accelerate\n",
    "!pip install langchain\n",
    "!pip install bitsandbytes\n",
    "# Update necessary libraries\n",
    "!pip install --upgrade langchain-huggingface langchain-core huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "sec_key=userdata.get('HF_TOKEN_API')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from google.colab import userdata\n",
    "sec_key=userdata.get('HF_HUB_TOKEN')\n",
    "#print(sec_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = sec_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,temperature=0.7)#max_length=128, token=sec_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"What is life?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "Question=\"Who is the best south african cricket player\"\n",
    "template=\"\"\"Question: {Question}\n",
    "Answer : Let's think step by step. \"\"\"\n",
    "prompt=PromptTemplate(template=template,input_variables=[\"Question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain=LLMChain(llm=llm,prompt=prompt)\n",
    "print(llm_chain.invoke(Question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id=\"gpt2\"\n",
    "model=AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe=pipeline(\"text-generation\",model=model,tokenizer=tokenizer,max_new_tokens=100)\n",
    "hf=HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.invoke(\"Loki is the best MCU character\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"gpt2\",\n",
    "    task=\"text-generation\",\n",
    "    device=0,\n",
    "    pipeline_kwargs={\"max_new_tokens\": 100},\n",
    ")\n",
    "# Verify GPU Availability: Before running the code, check if a GPU is available in\n",
    "#  your Colab environment. You can do this by going to \"Runtime\" -> \"Change runtime type\"\n",
    "#  and selecting \"T4 GPU\" or another available GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts  import PromptTemplate\n",
    "template=\"\"\"Question: {Question}\n",
    "\n",
    "Answer : Let's think step by step. \"\"\"\n",
    "prompt=PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=prompt|gpu_llm #another way of creating a chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Question=\"Is Loki better than Thor\"\n",
    "chain.invoke({\"Question:\",Question})\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
