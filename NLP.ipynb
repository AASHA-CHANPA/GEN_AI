{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxM2UUodDFAwmlCTKEAZVX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AASHA-CHANPA/GEN_AI/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --user -U nltk\n",
        "!pip install --user -U numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPngH2SzVbGy",
        "outputId": "a30c3b1e-a39e-4f68-b136-27fa29a8765b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: numpy in /root/.local/lib/python3.11/site-packages (2.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "cSD8D5dFDCWa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Tokenization (Text → Words/Sentences)"
      ],
      "metadata": {
        "id": "qUp6XhlMVHRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    nltk.download('punkt_tab')\n",
        "    # while the nltk library itself is installed, the necessary data files\n",
        "    #  (specifically the Punkt tokenizer models for English)\n",
        "    #  are not present in the locations NLTK is searching."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd016WBaE047",
        "outputId": "aa41eab9-b937-4dad-e350-9f5f37ab1385"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s1 = \"On a $50,000 mortgage of 30 years at 8 percent, the monthly payment would be $366.88.\"\n",
        "word_tokenize(s1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1-3WeWIEQNF",
        "outputId": "10e746fe-a8a4-4b6b-914f-1b241013cabc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['On',\n",
              " 'a',\n",
              " '$',\n",
              " '50,000',\n",
              " 'mortgage',\n",
              " 'of',\n",
              " '30',\n",
              " 'years',\n",
              " 'at',\n",
              " '8',\n",
              " 'percent',\n",
              " ',',\n",
              " 'the',\n",
              " 'monthly',\n",
              " 'payment',\n",
              " 'would',\n",
              " 'be',\n",
              " '$',\n",
              " '366.88',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "text = \"Hello! I’m learning NLP with NLTK. It's fun.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6c_lodPFUae",
        "outputId": "bf326d4d-8c65-479a-ffa7-3578b3b4cbf2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', '!', 'I', '’', 'm', 'learning', 'NLP', 'with', 'NLTK', '.', 'It', \"'s\", 'fun', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Noise Removal / Regex Cleaning (Remove punctuation, links, emojis)"
      ],
      "metadata": {
        "id": "b8wI7yXvVKLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = \"Hello #Aasha! I https://want.org to be more than friends with you! :)\"\n",
        "cleaned = re.sub(r\"https?://\\S+|[^A-Za-z\\s]\", \"\", text)\n",
        "print(cleaned)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Jxyy1j-GDtV",
        "outputId": "851252a5-0c47-42a2-c4c7-24a1fc82f74f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Aasha I  to be more than friends with you \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 3. Stopwords Removal (Eliminate common words like “the”, “is”)"
      ],
      "metadata": {
        "id": "Fn5OJMruVuqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    nltk.data.find('tcorpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKGc2iCxGkQl",
        "outputId": "eb2f6802-117b-4359-cd0c-d3294055da05"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered = [word for word in tokens if word.lower() not in stop_words]\n",
        "print(filtered)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhlMPuGZHDM6",
        "outputId": "94d766db-fa3d-4851-974c-1e89a72416b0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', '!', '’', 'learning', 'NLP', 'NLTK', '.', \"'s\", 'fun', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 4. Lexicon Normalization (Reduce words to root forms) - Stemming and Lemmatization"
      ],
      "metadata": {
        "id": "tLFD1yB7UlNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vy8aP9gBHOam",
        "outputId": "635a9eaa-4cfe-4d81-db7e-42a38db21627"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(stemmer.stem(\"running\"))      # run\n",
        "print(lemmatizer.lemmatize(\"running\", pos=\"v\"))  # run"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XelZSS6rHQ_g",
        "outputId": "06575ca8-4755-48c5-c6c9-96b9704c263d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n",
            "run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. POS Tagging (Part-of-Speech) (Understand word roles: noun, verb) and Helps in grammar, meaning, and context"
      ],
      "metadata": {
        "id": "oIS2MhRFUVLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    nltk.data.find('taggers/averaged_perceptron_tagger_eng')\n",
        "except LookupError:\n",
        "    nltk.download('averaged_perceptron_tagger_eng')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKAxYzOOHuZA",
        "outputId": "30f41862-cf23-4d9d-a1ae-4a82c734bfef"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk import pos_tag\n",
        "pos_tags = pos_tag(tokens)\n",
        "print(pos_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bffgItEHIJlj",
        "outputId": "e5e166b8-8b5f-4469-c302-f722db6d6a19"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Hello', 'NN'), ('!', '.'), ('I', 'PRP'), ('’', 'VBP'), ('m', 'JJ'), ('learning', 'VBG'), ('NLP', 'NNP'), ('with', 'IN'), ('NLTK', 'NNP'), ('.', '.'), ('It', 'PRP'), (\"'s\", 'VBZ'), ('fun', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    nltk.data.find('taggers/averaged_perceptron_tagger_eng')\n",
        "except LookupError:\n",
        "    nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "id": "6pgCpqj-Sb9v"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 6. Named Entity Recognition (NER)\n",
        " Use nltk.ne_chunk() on POS-tagged tokens\n",
        " Extract names, places, etc."
      ],
      "metadata": {
        "id": "yEmkeDhMSd91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    nltk.data.find('chunkers/maxent_ne_chunker_tab')\n",
        "except LookupError:\n",
        "    nltk.download('maxent_ne_chunker_tab')\n",
        "try:\n",
        "    nltk.data.find('corpora/words')\n",
        "except LookupError:\n",
        "    nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnUmWOkVSqhH",
        "outputId": "e94bf50d-8d57-456c-db20-a1daddabd4ee"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk import ne_chunk\n",
        "tree = ne_chunk(pos_tags)\n",
        "print(tree)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMnGvYIISKjP",
        "outputId": "9711019e-aced-443c-df51-17da27778913"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (GPE Hello/NN)\n",
            "  !/.\n",
            "  I/PRP\n",
            "  ’/VBP\n",
            "  m/JJ\n",
            "  learning/VBG\n",
            "  (ORGANIZATION NLP/NNP)\n",
            "  with/IN\n",
            "  (ORGANIZATION NLTK/NNP)\n",
            "  ./.\n",
            "  It/PRP\n",
            "  's/VBZ\n",
            "  fun/NN\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YDDBpE7ZTzre"
      }
    }
  ]
}